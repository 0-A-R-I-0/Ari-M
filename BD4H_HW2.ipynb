{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0-A-R-I-0/Ari-M/blob/main/BD4H_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1IxsYnj_Uy"
      },
      "source": [
        "# Environment setup\n",
        "The following code can help you install the packages required for the pyspark environment in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi87qFanj_U3",
        "outputId": "ad5f69c2-211c-4a3f-fdef-af949926f30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317124 sha256=612a4c455bfa90baff71428fb59e77c77519288331cd178af7d0ffbfab50af89\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/71/c8/6ffadf411f7456a87d125cf3b7735f091f24e56ba54dd17852\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbAVQ7Kuj_U5"
      },
      "source": [
        "The following is to mount Google drive files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "slFfnxlXj_U5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8367d910-113e-4418-bd9f-82ff6f4c3d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n",
            "Mounted at /content/gdrive\n",
            "Working Directory: /content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data\n"
          ]
        }
      ],
      "source": [
        "# View and modify the working path\n",
        "import os\n",
        "from google.colab import drive\n",
        "# View current working directory\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "# Change working directory to your file position\n",
        "path = \"/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data\"\n",
        "os.chdir(path)\n",
        "\n",
        "# Confirm the change\n",
        "print(\"Working Directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj9mBRyJj_U6"
      },
      "source": [
        "Create the test scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DqV4N9qzj_U7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_initial_score_df():\n",
        "    # Test names\n",
        "    test_names = ['TestEventStatistics', 'ETLTest']\n",
        "    # Initial scores set to 0\n",
        "    initial_scores = [0, 0]\n",
        "    # Creating the DataFrame\n",
        "    df = pd.DataFrame({'Test': test_names, 'Score': initial_scores})\n",
        "    return df\n",
        "\n",
        "# Create the initial DataFrame\n",
        "df_score = create_initial_score_df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORSD6kKTj_U7"
      },
      "source": [
        "# 2.1 Descriptive Statistics [15 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfF7dOTIj_U8",
        "outputId": "e0c1ce76-2dc8-4661-bfaa-0953efad2ac3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to compute event count metrics: 0.2793467044830322s\n",
            "+-----------------+---+-----+\n",
            "|              avg|min|  max|\n",
            "+-----------------+---+-----+\n",
            "|683.1552587646077|  1|12627|\n",
            "+-----------------+---+-----+\n",
            "\n",
            "+------------------+---+-----+\n",
            "|               avg|min|  max|\n",
            "+------------------+---+-----+\n",
            "|1027.7385229540919|  2|16829|\n",
            "+------------------+---+-----+\n",
            "\n",
            "Time to compute encounter count metrics: 0.2467796802520752s\n",
            "+------------------+------+---+---+\n",
            "|               avg|median|min|max|\n",
            "+------------------+------+---+---+\n",
            "|18.695492487479132|     9|  1|391|\n",
            "+------------------+------+---+---+\n",
            "\n",
            "+------------------+------+---+---+\n",
            "|               avg|median|min|max|\n",
            "+------------------+------+---+---+\n",
            "|24.839321357285428|    14|  1|375|\n",
            "+------------------+------+---+---+\n",
            "\n",
            "Time to compute record length metrics: 0.37967753410339355s\n",
            "+------------------+------+---+----+\n",
            "|               avg|median|min| max|\n",
            "+------------------+------+---+----+\n",
            "|194.70283806343906|    16|  0|3103|\n",
            "+------------------+------+---+----+\n",
            "\n",
            "+------------------+------+---+----+\n",
            "|               avg|median|min| max|\n",
            "+------------------+------+---+----+\n",
            "|157.04191616766468|    25|  0|5364|\n",
            "+------------------+------+---+----+\n",
            "\n",
            "Time to compute event count metrics: 0.5478627681732178s\n",
            "+------------+----------+\n",
            "|     eventid|diag_count|\n",
            "+------------+----------+\n",
            "|  DIAG320128|      1018|\n",
            "|  DIAG319835|       721|\n",
            "|  DIAG317576|       719|\n",
            "|DIAG42872402|       674|\n",
            "|  DIAG313217|       641|\n",
            "+------------+----------+\n",
            "\n",
            "+----------+---------+\n",
            "|   eventid|lab_count|\n",
            "+----------+---------+\n",
            "|LAB3009542|    66937|\n",
            "|LAB3000963|    57751|\n",
            "|LAB3023103|    57022|\n",
            "|LAB3018572|    54721|\n",
            "|LAB3007461|    53560|\n",
            "+----------+---------+\n",
            "\n",
            "+------------+---------+\n",
            "|     eventid|med_count|\n",
            "+------------+---------+\n",
            "|DRUG19095164|    12468|\n",
            "|DRUG43012825|    10389|\n",
            "|DRUG19049105|     9351|\n",
            "|DRUG19122121|     7586|\n",
            "|  DRUG956874|     7301|\n",
            "+------------+---------+\n",
            "\n",
            "+----------+----------+\n",
            "|   eventid|diag_count|\n",
            "+----------+----------+\n",
            "|DIAG320128|       416|\n",
            "|DIAG319835|       413|\n",
            "|DIAG313217|       377|\n",
            "|DIAG197320|       346|\n",
            "|DIAG132797|       297|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+---------+\n",
            "|   eventid|lab_count|\n",
            "+----------+---------+\n",
            "|LAB3009542|    32765|\n",
            "|LAB3023103|    28395|\n",
            "|LAB3000963|    28308|\n",
            "|LAB3018572|    27383|\n",
            "|LAB3016723|    27060|\n",
            "+----------+---------+\n",
            "\n",
            "+------------+---------+\n",
            "|     eventid|med_count|\n",
            "+------------+---------+\n",
            "|DRUG19095164|     6396|\n",
            "|DRUG43012825|     5451|\n",
            "|DRUG19049105|     4326|\n",
            "|  DRUG956874|     3962|\n",
            "|DRUG19122121|     3910|\n",
            "+------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "import time\n",
        "import pip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import datediff, to_date, max as max_, lit, col, collect_list, row_number, concat_ws, format_number, concat, monotonically_increasing_id\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "def read_csv(spark, file, schema):\n",
        "    return spark.read.csv(file, header=False, schema=schema)\n",
        "\n",
        "def split_alive_dead(events, mortality):\n",
        "    '''\n",
        "    param: spart dataframe events: [petientid, eventid, etimestamp, value] and dataframe mortality: [patientid, mtimestamp, label]\n",
        "    return: spark dataframe alive_evnets and dead_events\n",
        "\n",
        "    Task1: This function needs to be completed.\n",
        "    Split the events to two spark dataframes. One is for alive patients, and one is\n",
        "    for dead patients.\n",
        "    Variables returned from this function are passed as input DataFrame for later.\n",
        "    '''\n",
        "\n",
        "    # find patient ids from the mortality file\n",
        "    mortality_ids = mortality.select(\"patientid\").distinct()\n",
        "\n",
        "    # filter out the dead ids\n",
        "    dead_events = events.join(mortality_ids, on=\"patientid\", how=\"semi\")\n",
        "\n",
        "    # separate the alive/dead\n",
        "    alive_events = events.join(mortality_ids, on='patientid', how='left_anti')\n",
        "\n",
        "    return alive_events, dead_events\n",
        "\n",
        "\n",
        "def event_count_metrics(alive_events, dead_events):\n",
        "    '''\n",
        "    param: two spark DataFrame: alive_events, dead_events\n",
        "    return: two spark DataFrame\n",
        "\n",
        "    Task 2: Event count metrics\n",
        "    Compute average, min and max of event counts\n",
        "    for alive and dead patients respectively\n",
        "    +------+------+------+\n",
        "    |   avg|   min|  max |\n",
        "    +------+------+------+\n",
        "    |value1|value2|value3|\n",
        "    +------+------+------+\n",
        "    note:\n",
        "    1.please keep same column name as example showed before!\n",
        "    2.return two DataFrame for alive and dead patients' events respectively.\n",
        "    3.average computed with avg(), DO NOT round the results.\n",
        "    '''\n",
        "    '''\n",
        "        Group Data by patient Id\n",
        "        Count how many events each patient has\n",
        "        calculate mean, minimum, maximum, for the\n",
        "        entire dataframe\n",
        "    '''\n",
        "    def calculate_stats(df):\n",
        "        patient_counts = df.groupBy(\"patientid\").count()\n",
        "        stats = patient_counts.agg(\n",
        "            mean(\"count\").alias(\"avg\"),\n",
        "            min(\"count\").alias(\"min\"),\n",
        "            max_(\"count\").alias(\"max\")\n",
        "        )\n",
        "        return stats\n",
        "\n",
        "    alive_stats = calculate_stats(alive_events)\n",
        "    dead_stats = calculate_stats(dead_events)\n",
        "\n",
        "    return  alive_stats, dead_stats\n",
        "\n",
        "\n",
        "def encounter_count_metrics(alive_events, dead_events):\n",
        "    '''\n",
        "    param: two spark DataFrame: alive_events, dead_events\n",
        "    return: two spark DataFrame\n",
        "\n",
        "    Task3: Compute average, median, min and max of encounter counts\n",
        "    for alive and dead patients respectively\n",
        "    +------+--------+------+------+\n",
        "    |  avg | median | min  | max  |\n",
        "    +------+--------+------+------+\n",
        "    |value1| value2 |value3|value4|\n",
        "    +------+--------+------+------+\n",
        "    note:\n",
        "    1.keep alive dataframe and dead dateframe respectively, in this case, you will get 2 (1 for alive and 1 for dead) dataframe.\n",
        "    2.please keep same column name as example showed before!\n",
        "    3.average computed with mean(), DO NOT need to round the results.\n",
        "    4.for the median section, when the array is even, do not use the average of the two middle elements, you can choose\n",
        "      the smallest value around the midpoint of an even array. Or, 'percentile_approx' func may useful, the link:\n",
        "      https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.percentile_approx.html.\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "    Group data by patient id\n",
        "    drop events that happened on the same day\n",
        "    count how many unique date events occured\n",
        "    return statistics oon the dataaframes\n",
        "    '''\n",
        "\n",
        "    def calc_stats(df):\n",
        "        timestamp_col = None\n",
        "        for col in df.columns:\n",
        "            if col.endswith('timestamp'):\n",
        "                timestamp_col = col\n",
        "                break\n",
        "        if timestamp_col == None:\n",
        "            raise_error ('Timestamp should not be None')\n",
        "        patient_counts = (df\n",
        "                          .groupBy(\"patientid\")\n",
        "                          .agg(\n",
        "                                countDistinct(timestamp_col)\n",
        "                                .alias('encounter_count')\n",
        "                              )\n",
        "                          )\n",
        "\n",
        "        stats = patient_counts.agg(\n",
        "            mean(\"encounter_count\").alias(\"avg\"),\n",
        "            percentile_approx(\"encounter_count\", 0.5).alias(\"median\"),\n",
        "            min(\"encounter_count\").alias(\"min\"),\n",
        "            max(\"encounter_count\").alias(\"max\")\n",
        "        )\n",
        "        return stats\n",
        "\n",
        "    alive_encounter_res = calc_stats(alive_events)\n",
        "    dead_encounter_res = calc_stats(dead_events)\n",
        "\n",
        "    return alive_encounter_res, dead_encounter_res\n",
        "\n",
        "def record_length_metrics(alive_events, dead_events):\n",
        "    '''\n",
        "    param: two spark DataFrame:alive_events, dead_events\n",
        "    return: two spark DataFrame\n",
        "\n",
        "    Task4: Record length metrics\n",
        "    Compute average, median, min and max of record lengths\n",
        "    for alive and dead patients respectively\n",
        "    +------+--------+------+------+\n",
        "    |  avg | median | min  | max  |\n",
        "    +------+--------+------+------+\n",
        "    |value1| value2 |value3|value4|\n",
        "    +------+--------+------+------+\n",
        "    note:\n",
        "    1.keep alive dataframe and dead dateframe respectively, in this case, you will get 2 (1 for alive and 1 for dead) dataframe.\n",
        "    2.please keep same column name as example showed before!\n",
        "    3.average computed with mean(), DO NOT round the results.\n",
        "\n",
        "    '''\n",
        "    def calc_stats(df):\n",
        "        patient_min_max = df.groupBy('patientid').agg(\n",
        "            min('etimestamp').alias('min'),\n",
        "            max('etimestamp').alias('max')\n",
        "        )\n",
        "        patient_r_length = patient_min_max.withColumn(\n",
        "            'record_length',\n",
        "            datediff(to_date(col('max')), to_date(col('min')))\n",
        "        )\n",
        "        # the stats\n",
        "        stats = patient_r_length.agg(\n",
        "            mean('record_length').alias('avg'),\n",
        "            percentile_approx('record_length', 0.5).alias('median'),\n",
        "            min('record_length').alias('min'),\n",
        "            max('record_length').alias('max')\n",
        "        )\n",
        "        return stats\n",
        "\n",
        "    return calc_stats(alive_events), calc_stats(dead_events)\n",
        "\n",
        "def Common(alive_events, dead_events):\n",
        "    '''\n",
        "    param: two spark DataFrame: alive_events, dead_events\n",
        "    return: six spark DataFrame\n",
        "    Task 5: Common diag/lab/med\n",
        "    Compute the 5 most frequently occurring diag/lab/med\n",
        "    for alive and dead patients respectively\n",
        "    +------------+----------+\n",
        "    |   eventid  |diag_count|\n",
        "    +------------+----------+\n",
        "    |  DIAG999999|      9999|\n",
        "    |  DIAG999999|      9999|\n",
        "    |  DIAG999999|      9999|\n",
        "    |  DIAG999999|      9999|\n",
        "    |  DIAG999999|      9999|\n",
        "    +------------+----------+\n",
        "\n",
        "    +------------+----------+\n",
        "    |   eventid  | lab_count|\n",
        "    +------------+----------+\n",
        "    |  LAB999999 |      9999|\n",
        "    |  LAB999999 |      9999|\n",
        "    |  LAB999999 |      9999|\n",
        "    |  LAB999999 |      9999|\n",
        "    +------------+----------+\n",
        "\n",
        "    +------------+----------+\n",
        "    |   eventid  | med_count|\n",
        "    +------------+----------+\n",
        "    |  DRUG999999|      9999|\n",
        "    |  DRUG999999|      9999|\n",
        "    |  DRUG999999|      9999|\n",
        "    |  DRUG999999|      9999|\n",
        "    |  DRUG999999|      9999|\n",
        "    +------------+----------+\n",
        "    note:\n",
        "    1.keep alive dataframe and dead dateframe respectively, in this case, you will get 6 (3 for alive and 3 for dead) dataframe.\n",
        "    2.please keep same column name as example showed before!\n",
        "    '''\n",
        "    a_df = alive_events.groupBy('eventid').count().sort('count', ascending=False)\n",
        "    d_df = dead_events.groupBy('eventid').count().sort('count', ascending=False)\n",
        "\n",
        "    alive_diag = get_top5_events(a_df, 'DIAG', 'count')\n",
        "    dead_diag = get_top5_events(d_df, 'DIAG', 'count')\n",
        "\n",
        "    alive_lab = get_top5_events(a_df, 'LAB', 'count')\n",
        "    dead_lab = get_top5_events(d_df, 'LAB', 'count')\n",
        "\n",
        "    alive_med = (get_top5_events(a_df, 'DRUG', 'count')\n",
        "                .withColumnRenamed('drug_count','med_count'))\n",
        "\n",
        "    dead_med = (get_top5_events(d_df, 'DRUG', 'count')\n",
        "              .withColumnRenamed('drug_count','med_count'))\n",
        "\n",
        "    return alive_diag, alive_lab, alive_med, dead_diag, dead_lab, dead_med\n",
        "\n",
        "def get_top5_events(df, prefix, count_col_name):\n",
        "    return (df\n",
        "            .filter(df.eventid.startswith(prefix))\n",
        "            .limit(5)\n",
        "            .withColumnRenamed(count_col_name, prefix.lower() + '_count')\n",
        "            )\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    DO NOT MODIFY THIS FUNCTION.\n",
        "    '''\n",
        "\n",
        "    # You may change the following path variable in coding but switch it back when submission.\n",
        "    path1 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data/events.csv' #/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/data/events.csv'\n",
        "    path2 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data/mortality.csv' # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/data/mortality.csv'\n",
        "\n",
        "    schema1 = StructType([\n",
        "        StructField(\"patientid\", IntegerType(), True),\n",
        "        StructField(\"eventid\", StringType(), True),\n",
        "        StructField(\"eventdesc\", StringType(), True),\n",
        "        StructField(\"timestamp\", StringType(), True),\n",
        "        StructField(\"value\", FloatType(), True)])\n",
        "\n",
        "    schema2 = StructType([\n",
        "        StructField(\"patientid\", IntegerType(), True),\n",
        "        StructField(\"timestamp\", StringType(), True),\n",
        "        StructField(\"label\", IntegerType(), True)])\n",
        "\n",
        "    events = read_csv(spark, path1, schema1)\n",
        "    events = events.select(events.patientid, events.eventid, to_date(events.timestamp).alias(\"etimestamp\"), events.value)\n",
        "\n",
        "    mortality = read_csv(spark, path2, schema2)\n",
        "    mortality = mortality.select(mortality.patientid, to_date(mortality.timestamp).alias(\"mtimestamp\"), mortality.label)\n",
        "\n",
        "    alive_events, dead_events = split_alive_dead(events, mortality)\n",
        "\n",
        "    #Compute the event count metrics\n",
        "    start_time = time.time()\n",
        "    alive_statistics, dead_statistics = event_count_metrics(alive_events, dead_events)\n",
        "    end_time = time.time()\n",
        "    print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "    alive_statistics.show()\n",
        "    dead_statistics.show()\n",
        "\n",
        "    #Compute the encounter count metrics\n",
        "    start_time = time.time()\n",
        "    alive_encounter_res, dead_encounter_res = encounter_count_metrics(alive_events, dead_events)\n",
        "    end_time = time.time()\n",
        "    print((\"Time to compute encounter count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "    alive_encounter_res.show()\n",
        "    dead_encounter_res.show()\n",
        "\n",
        "    #Compute record length metrics\n",
        "    start_time = time.time()\n",
        "    alive_recordlength_res, dead_recordlength_res = record_length_metrics(alive_events, dead_events)\n",
        "    end_time = time.time()\n",
        "    print((\"Time to compute record length metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "    alive_recordlength_res.show()\n",
        "    dead_recordlength_res.show()\n",
        "\n",
        "    #Compute Common metrics\n",
        "    start_time = time.time()\n",
        "    alive_diag, alive_lab, alive_med, dead_diag, dead_lab, dead_med = Common(alive_events, dead_events)\n",
        "    end_time = time.time()\n",
        "    print((\"Time to compute event count metrics: \" + str(end_time - start_time) + \"s\"))\n",
        "    alive_diag.show()\n",
        "    alive_lab.show()\n",
        "    alive_med.show()\n",
        "    dead_diag.show()\n",
        "    dead_lab.show()\n",
        "    dead_med.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNfSRvuMj_U9"
      },
      "source": [
        "#### Run this block to get the descriptive statistics test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "08o1vO22j_U-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e02401-6574-49ed-c6d2-0821ea834189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common\n",
            "1 1\n",
            "3013682 3013682\n",
            "4 4\n",
            "2 2\n",
            "55 55\n",
            "956874 956874\n",
            "Event Stats passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encounter count passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "event count\n",
            "426.5 426.5\n",
            "299 299\n",
            "554 554\n",
            "636.66666666 636.6666666666666\n",
            "543 543\n",
            "812 812\n",
            "Event count passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Record length passed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 23.236s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed! Total Score: 15/15\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "\n",
        "class TestEventStatistics(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.spark = SparkSession.builder.appName('Test Event Statistics').getOrCreate()\n",
        "\n",
        "        # You may change the following path variable in coding but switch it back when submission\n",
        "        cls.path1 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv' # '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv'\n",
        "        cls.path2 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv' # '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv'\n",
        "\n",
        "        cls.schema1 = StructType([\n",
        "            StructField(\"patientid\", IntegerType(), True),\n",
        "            StructField(\"eventid\", StringType(), True),\n",
        "            StructField(\"eventdesc\", StringType(), True),\n",
        "            StructField(\"timestamp\", StringType(), True),\n",
        "            StructField(\"value\", FloatType(), True)\n",
        "        ])\n",
        "\n",
        "        cls.schema2 = StructType([\n",
        "            StructField(\"patientid\", IntegerType(), True),\n",
        "            StructField(\"timestamp\", StringType(), True),\n",
        "            StructField(\"label\", IntegerType(), True)\n",
        "        ])\n",
        "\n",
        "        cls.events = read_csv(cls.spark, cls.path1, cls.schema1)\n",
        "        cls.events = cls.events.select(cls.events.patientid, cls.events.eventid, to_date(cls.events.timestamp).alias(\"etimestamp\"), cls.events.value)\n",
        "\n",
        "        cls.mortality = read_csv(cls.spark, cls.path2, cls.schema2)\n",
        "        cls.mortality = cls.mortality.select(cls.mortality.patientid, to_date(cls.mortality.timestamp).alias(\"mtimestamp\"), cls.mortality.label)\n",
        "\n",
        "        cls.alive_events, cls.dead_events = split_alive_dead(cls.events, cls.mortality)\n",
        "        cls.score = 0  # Initialize the score\n",
        "\n",
        "    def test_event_count(self):\n",
        "        try:\n",
        "            actual_value_1 = 426.5\n",
        "            actual_value_2 = 299\n",
        "            actual_value_3 = 554\n",
        "            actual_value_4 = 636.66666666\n",
        "            actual_value_5 = 543\n",
        "            actual_value_6 = 812\n",
        "\n",
        "            alive_statistics, dead_statistics = event_count_metrics(self.alive_events, self.dead_events)\n",
        "\n",
        "            avg_alive = alive_statistics.select('avg').collect()[0][0]\n",
        "            min_alive = alive_statistics.select('min').collect()[0][0]\n",
        "            max_alive = alive_statistics.select('max').collect()[0][0]\n",
        "            avg_dead = dead_statistics.select('avg').collect()[0][0]\n",
        "            min_dead = dead_statistics.select('min').collect()[0][0]\n",
        "            max_dead = dead_statistics.select('max').collect()[0][0]\n",
        "\n",
        "            print('event count')\n",
        "            print(actual_value_1, avg_alive)\n",
        "            print(actual_value_2, min_alive)\n",
        "            print(actual_value_3, max_alive)\n",
        "            print(actual_value_4, avg_dead)\n",
        "            print(actual_value_5, min_dead)\n",
        "            print(actual_value_6, max_dead)\n",
        "\n",
        "\n",
        "            self.assertAlmostEqual(avg_alive, actual_value_1, places=2, msg=\"UNEQUAL in live_avg, Expected:%s, Actual:%s\" % (actual_value_1, avg_alive))\n",
        "            self.assertAlmostEqual(min_alive, actual_value_2, places=2, msg=\"UNEQUAL in live_min, Expected:%s, Actual:%s\" % (actual_value_2, min_alive))\n",
        "            self.assertAlmostEqual(max_alive, actual_value_3, places=2, msg=\"UNEQUAL in live_max, Expected:%s, Actual:%s\" % (actual_value_3, max_alive))\n",
        "            self.assertAlmostEqual(avg_dead, actual_value_4, places=2, msg=\"UNEQUAL in dead_avg, Expected:%s, Actual:%s\" % (actual_value_4, avg_dead))\n",
        "            self.assertAlmostEqual(min_dead, actual_value_5, places=2, msg=\"UNEQUAL in dead_min, Expected:%s, Actual:%s\" % (actual_value_5, min_dead))\n",
        "            self.assertAlmostEqual(max_dead, actual_value_6, places=2, msg=\"UNEQUAL in dead_max, Expected:%s, Actual:%s\" % (actual_value_6, max_dead))\n",
        "            TestEventStatistics.score += 3  # Increment score\n",
        "            print('Event count passed')\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_encounter_count(self):\n",
        "        try:\n",
        "            expected_value_1 = 19.0\n",
        "            expected_value_2 = 7\n",
        "            expected_value_3 = 7\n",
        "            expected_value_4 = 31\n",
        "            expected_value_5 = 15.0\n",
        "            expected_value_6 = 11\n",
        "            expected_value_7 = 11\n",
        "            expected_value_8 = 23\n",
        "\n",
        "            alive_encounter_res, dead_encounter_res = encounter_count_metrics(self.alive_events, self.dead_events)\n",
        "\n",
        "            avg_alive_encounter_count = alive_encounter_res.select('avg').collect()[0][0]\n",
        "            median_alive_encounter_count = alive_encounter_res.select('median').collect()[0][0]\n",
        "            min_alive_encounter_count = alive_encounter_res.select('min').collect()[0][0]\n",
        "            max_alive_encounter_count = alive_encounter_res.select('max').collect()[0][0]\n",
        "            avg_dead_encounter_count = dead_encounter_res.select('avg').collect()[0][0]\n",
        "            median_dead_encounter_count = dead_encounter_res.select('median').collect()[0][0]\n",
        "            min_dead_encounter_count = dead_encounter_res.select('min').collect()[0][0]\n",
        "            max_dead_encounter_count = dead_encounter_res.select('max').collect()[0][0]\n",
        "\n",
        "            self.assertAlmostEqual(avg_alive_encounter_count, expected_value_1, places=2)\n",
        "            self.assertAlmostEqual(median_alive_encounter_count, expected_value_2, places=2)\n",
        "            self.assertAlmostEqual(min_alive_encounter_count, expected_value_3, places=2)\n",
        "            self.assertAlmostEqual(max_alive_encounter_count, expected_value_4, places=2)\n",
        "            self.assertAlmostEqual(avg_dead_encounter_count, expected_value_5, places=2)\n",
        "            self.assertAlmostEqual(median_dead_encounter_count, expected_value_6, places=2)\n",
        "            self.assertAlmostEqual(min_dead_encounter_count, expected_value_7, places=2)\n",
        "            self.assertAlmostEqual(max_dead_encounter_count, expected_value_8, places=2)\n",
        "            TestEventStatistics.score += 3  # Increment score\n",
        "            print('Encounter count passed')\n",
        "\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_record_length(self):\n",
        "        try:\n",
        "            expected_value_1 = 1061.0\n",
        "            expected_value_2 = 6\n",
        "            expected_value_3 = 6\n",
        "            expected_value_4 = 2116\n",
        "            expected_value_5 = 25.66666666\n",
        "            expected_value_6 = 21\n",
        "            expected_value_7 = 10\n",
        "            expected_value_8 = 46\n",
        "\n",
        "            alive_recordlength_res, dead_recordlength_res = record_length_metrics(self.alive_events, self.dead_events)\n",
        "\n",
        "            avg_alive_rec_len = alive_recordlength_res.select('avg').collect()[0][0]\n",
        "            median_alive_rec_len = alive_recordlength_res.select('median').collect()[0][0]\n",
        "            min_alive_rec_len = alive_recordlength_res.select('min').collect()[0][0]\n",
        "            max_alive_rec_len = alive_recordlength_res.select('max').collect()[0][0]\n",
        "            avg_dead_rec_len = dead_recordlength_res.select('avg').collect()[0][0]\n",
        "            median_dead_rec_len = dead_recordlength_res.select('median').collect()[0][0]\n",
        "            min_dead_rec_len = dead_recordlength_res.select('min').collect()[0][0]\n",
        "            max_dead_rec_len = dead_recordlength_res.select('max').collect()[0][0]\n",
        "\n",
        "            self.assertAlmostEqual(avg_alive_rec_len, expected_value_1, places=2)\n",
        "            self.assertAlmostEqual(median_alive_rec_len, expected_value_2, places=2)\n",
        "            self.assertAlmostEqual(min_alive_rec_len, expected_value_3, places=2)\n",
        "            self.assertAlmostEqual(max_alive_rec_len, expected_value_4, places=2)\n",
        "            self.assertAlmostEqual(avg_dead_rec_len, expected_value_5, places=2)\n",
        "            self.assertAlmostEqual(median_dead_rec_len, expected_value_6, places=2)\n",
        "            self.assertAlmostEqual(min_dead_rec_len, expected_value_7, places=2)\n",
        "            self.assertAlmostEqual(max_dead_rec_len, expected_value_8, places=2)\n",
        "            TestEventStatistics.score += 3  # Increment score\n",
        "            print('Record length passed')\n",
        "\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_Common(self):\n",
        "        try:\n",
        "            expected_value_1 = 1\n",
        "            expected_value_2 = 3013682\n",
        "            expected_value_3 = 4\n",
        "            expected_value_4 = 2\n",
        "            expected_value_5 = 55\n",
        "            expected_value_6 = 956874\n",
        "\n",
        "            alive_diag, alive_lab, alive_med, dead_diag, dead_lab, dead_med = Common(self.alive_events, self.dead_events)\n",
        "\n",
        "            alive_diag_count = alive_diag.collect()[1][1]\n",
        "            alive_event_id = alive_lab.collect()[2][0][3:]\n",
        "            alive_med_count = alive_med.collect()[3][1]\n",
        "            dead_event_id = dead_diag.collect()[0][1]\n",
        "            dead_lab_count = dead_lab.collect()[4][1]\n",
        "            dead_med_count = dead_med.collect()[0][0][4:]\n",
        "            print('Common')\n",
        "            print(expected_value_1, alive_diag_count)\n",
        "            print(expected_value_2, int(alive_event_id))\n",
        "            print(expected_value_3, alive_med_count)\n",
        "            print(expected_value_4, dead_event_id)\n",
        "            print(expected_value_5, dead_lab_count)\n",
        "            print(expected_value_6, int(dead_med_count))\n",
        "\n",
        "\n",
        "            self.assertAlmostEqual(expected_value_1, alive_diag_count, places=2)\n",
        "            self.assertAlmostEqual(expected_value_2, int(alive_event_id), places=2)\n",
        "            self.assertAlmostEqual(expected_value_3, alive_med_count, places=2)\n",
        "            self.assertAlmostEqual(expected_value_4, dead_event_id, places=2)\n",
        "            self.assertAlmostEqual(expected_value_5, dead_lab_count, places=2)\n",
        "            self.assertAlmostEqual(expected_value_6, int(dead_med_count), places=2)\n",
        "            print('Event Stats passed')\n",
        "\n",
        "            TestEventStatistics.score += 6  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        cls.spark.stop()\n",
        "        if cls.score == 15:  # All tests passed\n",
        "            print(f\"All tests passed! Total Score: {cls.score}/15\")\n",
        "        else:\n",
        "            print(f\"Total Score: {cls.score}/15\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create a test suite\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestEventStatistics)\n",
        "\n",
        "    # Run the test suite\n",
        "    unittest.TextTestRunner().run(suite)\n",
        "    df_score.loc[df_score['Test'] == 'TestEventStatistics', 'Score'] = TestEventStatistics.score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvWG1Hhdj_U_"
      },
      "source": [
        "# 2.2 Transform data [28 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GlUZs6hdj_U_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035213d1-6126-4f17-d84c-b6042999aac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index_dates\n",
            "+---------+----------+\n",
            "|patientid|index_date|\n",
            "+---------+----------+\n",
            "|    13905|1999-12-31|\n",
            "|    18676|2000-01-04|\n",
            "|    20301|2002-07-09|\n",
            "|    20459|2000-09-19|\n",
            "|     5206|2000-08-04|\n",
            "+---------+----------+\n",
            "\n",
            "filtered\n",
            "+---------+----------+----------+-----+\n",
            "|patientid|   eventid|etimestamp|value|\n",
            "+---------+----------+----------+-----+\n",
            "|    20459|LAB3007461|1995-04-24|130.0|\n",
            "|    20459|LAB3023103|1995-04-25|  3.5|\n",
            "|    20459|LAB3007220|1995-04-25| 60.0|\n",
            "|    20459|LAB3033236|1995-04-25| NULL|\n",
            "|    20459|LAB3007461|1995-04-25|119.0|\n",
            "|    20459|LAB3013682|1995-04-25| 11.0|\n",
            "|    20459|LAB3016723|1995-04-25|  1.3|\n",
            "|    20459|LAB3023103|1995-04-25|  3.8|\n",
            "|    20459|LAB3007220|1995-04-25| 59.0|\n",
            "|    20459|LAB3033236|1995-04-25| NULL|\n",
            "|    20459|LAB3009542|1995-04-25| 29.6|\n",
            "|    20459|LAB3007461|1995-04-25|156.0|\n",
            "|    20459|LAB3013682|1995-08-23| 21.0|\n",
            "|    20459|LAB3016723|1995-08-23|  1.2|\n",
            "|    20459|LAB3013603|1995-08-23|  0.6|\n",
            "|    20459|LAB3013682|1996-02-28| 18.0|\n",
            "|    20459|LAB3016723|1996-02-28|  1.5|\n",
            "|    20459|LAB3013603|1996-02-28|  0.5|\n",
            "|    20459|LAB3013682|1996-09-11| 15.0|\n",
            "|    20459|LAB3016723|1996-09-11|  1.5|\n",
            "+---------+----------+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "agg_events\n",
            "+---------+------------+-------------+\n",
            "|patientid|     eventid|feature_value|\n",
            "+---------+------------+-------------+\n",
            "|     5206|  DIAG192767|            1|\n",
            "|     5206|  DIAG193598|            1|\n",
            "|     5206|  DIAG194200|            1|\n",
            "|     5206|  DIAG197320|            1|\n",
            "|     5206|  DIAG434894|            1|\n",
            "|     5206|  DIAG437149|            1|\n",
            "|     5206|  DIAG440603|            1|\n",
            "|     5206|  DIAG442531|            1|\n",
            "|     5206|   DIAG79864|            1|\n",
            "|     5206|   DIAG81151|            1|\n",
            "|     5206| DRUG1000560|            2|\n",
            "|     5206| DRUG1126658|            6|\n",
            "|     5206| DRUG1742253|            2|\n",
            "|     5206|DRUG19012125|            1|\n",
            "|     5206|DRUG19049105|            1|\n",
            "|     5206|DRUG19065818|            1|\n",
            "|     5206|DRUG19079524|            1|\n",
            "|     5206|DRUG19093848|            2|\n",
            "|     5206|DRUG43012825|            1|\n",
            "|     5206|  DRUG948078|            1|\n",
            "+---------+------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "event_map\n",
            "+----------+-----------+\n",
            "|   eventid|event_index|\n",
            "+----------+-----------+\n",
            "|DIAG132797|          0|\n",
            "|DIAG135214|          1|\n",
            "|DIAG137829|          2|\n",
            "|DIAG141499|          3|\n",
            "|DIAG192767|          4|\n",
            "|DIAG193598|          5|\n",
            "|DIAG194200|          6|\n",
            "|DIAG196236|          7|\n",
            "|DIAG197320|          8|\n",
            "|DIAG198185|          9|\n",
            "|DIAG201826|         10|\n",
            "|DIAG252663|         11|\n",
            "|DIAG255573|         12|\n",
            "|DIAG256723|         13|\n",
            "|DIAG313217|         14|\n",
            "|DIAG315286|         15|\n",
            "|DIAG315295|         16|\n",
            "|DIAG317002|         17|\n",
            "|DIAG317576|         18|\n",
            "|DIAG318800|         19|\n",
            "+----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "normalized\n",
            "+---------+------------+------------------------+\n",
            "|patientid|     eventid|normalized_feature_value|\n",
            "+---------+------------+------------------------+\n",
            "|     5206|  DIAG192767|                     1.0|\n",
            "|     5206|  DIAG193598|                     1.0|\n",
            "|     5206|  DIAG194200|                     1.0|\n",
            "|     5206|  DIAG197320|                     1.0|\n",
            "|     5206|  DIAG434894|                     1.0|\n",
            "|     5206|  DIAG437149|                     1.0|\n",
            "|     5206|  DIAG440603|                     1.0|\n",
            "|     5206|  DIAG442531|                     1.0|\n",
            "|     5206|   DIAG79864|                     1.0|\n",
            "|     5206|   DIAG81151|                     1.0|\n",
            "|     5206| DRUG1000560|                     1.0|\n",
            "|     5206| DRUG1126658|                     1.0|\n",
            "|     5206| DRUG1742253|                     0.5|\n",
            "|     5206|DRUG19012125|                     1.0|\n",
            "|     5206|DRUG19049105|                   0.167|\n",
            "|     5206|DRUG19065818|                     1.0|\n",
            "|     5206|DRUG19079524|                     0.2|\n",
            "|     5206|DRUG19093848|                   0.182|\n",
            "|     5206|DRUG43012825|                     0.1|\n",
            "|     5206|  DRUG948078|                     0.5|\n",
            "+---------+------------+------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "svmlight\n",
            "+---------+--------------------+\n",
            "|patientid|      sparse_feature|\n",
            "+---------+--------------------+\n",
            "|     5206|[4:1.000, 5:1.000...|\n",
            "|    13905|[1:1.000, 11:1.00...|\n",
            "|    18676|[0:1.000, 2:1.000...|\n",
            "|    20301|[10:1.000, 12:1.0...|\n",
            "|    20459|[147:0.250, 148:1...|\n",
            "+---------+--------------------+\n",
            "\n",
            "svmlight samples\n",
            "+---------+--------------------+--------------------+\n",
            "|patientid|      sparse_feature|        save_feature|\n",
            "+---------+--------------------+--------------------+\n",
            "|     5206|[4:1.000, 5:1.000...|0 4:1.000 5:1.000...|\n",
            "|    13905|[1:1.000, 11:1.00...|1 1:1.000 11:1.00...|\n",
            "|    18676|[0:1.000, 2:1.000...|1 0:1.000 2:1.000...|\n",
            "|    20301|[10:1.000, 12:1.0...|1 10:1.000 12:1.0...|\n",
            "|    20459|[147:0.250, 148:1...|0 147:0.250 148:1...|\n",
            "+---------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import operator\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import datediff, to_date, max as max_, lit, date_sub, col, collect_list, row_number, concat_ws, format_number, concat, monotonically_increasing_id, sort_array, struct, asc\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName('Read CSV File into DataFrame').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "def read_csv(spark, file, schema):\n",
        "    return spark.read.csv(file, header=False, schema=schema)\n",
        "\n",
        "def calculate_index_dates(events, mortality):\n",
        "    '''\n",
        "    INPUT1:\n",
        "    events_df read from events.csv\n",
        "    e.g.\n",
        "    +---------+------------+----------+-----+\n",
        "    |patientid|     eventid|etimestamp|value|\n",
        "    +---------+------------+----------+-----+\n",
        "    |    20459|DIAG42872402|1994-12-04|  1.0|\n",
        "\n",
        "    INPUT2:\n",
        "    mortality_df read from mortality.csv\n",
        "    +---------+----------+-----+\n",
        "    |patientid|mtimestamp|label|\n",
        "    +---------+----------+-----+\n",
        "    |    13905|2000-01-30|    1|\n",
        "\n",
        "    OUTPUT:\n",
        "    index_df\n",
        "    index_date is datetime.date format\n",
        "    e.g.\n",
        "    +---------+----------+\n",
        "    |patientid|index_date|\n",
        "    +---------+----------+\n",
        "    |    20459|2000-09-19|\n",
        "    |    13905|1999-12-31|\n",
        "    +---------+----------+\n",
        "    '''\n",
        "    '''\n",
        "    FInd the dead patient ids\n",
        "    find the mostrecent timestamps for each patinet\n",
        "    For the dead patient ids substract 30 days from the day they died.\n",
        "    join the two dataframes to get the index date\n",
        "    '''\n",
        "    #get dead patients index date\n",
        "    dead_index_dates = mortality.withColumn(\n",
        "        'index_date', date_sub('mtimestamp', 30)\n",
        "    ).select('patientid', 'index_date')\n",
        "\n",
        "\n",
        "    # get alive patients final timestamp\n",
        "    alive_index_dates = (events\n",
        "                         .join(mortality, on='patientid', how='left_anti')\n",
        "                         .groupBy('patientid')\n",
        "                         .agg(\n",
        "                             max_('etimestamp')\n",
        "                             .alias('index_date')\n",
        "                             )\n",
        "                        )\n",
        "\n",
        "    # join the two together\n",
        "    df = dead_index_dates.union(alive_index_dates).select('patientid', 'index_date')\n",
        "\n",
        "    return df\n",
        "\n",
        "def filter_events(events, index_dates):\n",
        "    # TODO: filtered events should have the same input column of original events, select the corresponding columns and revise test as well\n",
        "    '''\n",
        "    INPUT:\n",
        "    events: created events df, e.g.\n",
        "    +---------+------------+----------+-----+\n",
        "    |patientid|     eventid|etimestamp|value|\n",
        "    +---------+------------+----------+-----+\n",
        "    |    20459|DIAG42872402|1994-12-04|  1.0|\n",
        "    +---------+------------+----------+-----+\n",
        "\n",
        "    index_dates: created index_date df, e.g\n",
        "    +---------+----------+\n",
        "    |patientid|index_date|\n",
        "    +---------+----------+\n",
        "    |    20459|2000-09-19|\n",
        "    +---------+----------+\n",
        "\n",
        "    OUTPUT:\n",
        "    filtered: e.g.\n",
        "    +---------+--------------+----------+-----+\n",
        "    |patientid|   eventid    |etimestamp|value|\n",
        "    +---------+--------------+----------+-----+\n",
        "    |    20459|'DIAG42872404'|1999-12-04|  1.0|\n",
        "    |    19992|'DIAG42872403'|1995-12-04|  1.0|\n",
        "    +---------+--------------+----------+-----+\n",
        "    '''\n",
        "    '''\n",
        "    Subract 2000 days from all index dates\n",
        "    join the events df by patient id\n",
        "    filter all events by if they are within the range of min/index date\n",
        "    '''\n",
        "\n",
        "    # Remove the events that are not in the observation window\n",
        "    df = (events\n",
        "        .join(index_dates, on='patientid', how='inner')\n",
        "        .filter(col('etimestamp') <= col('index_date'))\n",
        "        .filter(col('etimestamp') >= date_sub(col('index_date'), 2000))\n",
        "    ).select(events.columns)\n",
        "\n",
        "\n",
        "    return df.select('patientid', 'eventid', 'etimestamp', 'value')\n",
        "\n",
        "def aggregate_events(filtered):\n",
        "    '''\n",
        "    INPUT:\n",
        "    filtered\n",
        "    e.g.\n",
        "    +---------+----------+----------+-----+\n",
        "    |patientid|   eventid|etimestamp|value|\n",
        "    +---------+----------+----------+-----+\n",
        "    |    20459|LAB3013603|2000-09-19|  0.6|\n",
        "\n",
        "    OUTPUT:\n",
        "    patient_features\n",
        "    e.g.\n",
        "    +---------+------------+-------------+\n",
        "    |patientid|     eventid|feature_value|\n",
        "    +---------+------------+-------------+\n",
        "    |     5206|DRUG19065818|            1|\n",
        "    |     5206|  LAB3021119|            1|\n",
        "    |    20459|  LAB3013682|           11|\n",
        "    +---------+------------+-------------+\n",
        "    '''\n",
        "    '''\n",
        "    select the relevant columns\n",
        "    groupby patient id and event id\n",
        "    count the number of events for each patient\n",
        "    '''\n",
        "    # Output columns should be (patientid, eventid, feature_value)\n",
        "\n",
        "    agg_events = (filtered\n",
        "          .select('patientid', 'eventid', 'etimestamp')\n",
        "          .groupBy('patientid', 'eventid')\n",
        "          .agg(\n",
        "              count('etimestamp').alias('feature_value')\n",
        "              )\n",
        "        )\n",
        "\n",
        "\n",
        "    return (agg_events\n",
        "            .select('patientid', 'eventid', 'feature_value')\n",
        "            .orderBy('patientid', 'eventid'))\n",
        "\n",
        "def generate_feature_mapping(agg_events):\n",
        "    '''\n",
        "    INPUT:\n",
        "    agg_events\n",
        "    e.g.\n",
        "    +---------+------------+-------------+\n",
        "    |patientid|     eventid|feature_value|\n",
        "    +---------+------------+-------------+\n",
        "    |     5206|DRUG19065818|            1|\n",
        "    |     5206|  LAB3021119|            1|\n",
        "    |    20459|  LAB3013682|           11|\n",
        "    +---------+------------+-------------+\n",
        "\n",
        "    OUTPUT:\n",
        "    event_map\n",
        "    e.g.\n",
        "    +----------+-----------+\n",
        "    |   eventid|event_index|\n",
        "    +----------+-----------+\n",
        "    |DIAG132797|          0|\n",
        "    |DIAG135214|          1|\n",
        "    |DIAG137829|          2|\n",
        "    |DIAG141499|          3|\n",
        "    |DIAG192767|          4|\n",
        "    |DIAG193598|          5|\n",
        "    +----------+-----------+\n",
        "    '''\n",
        "    '''\n",
        "    select only the unique event ids\n",
        "    sortevent ids\n",
        "    using monotoncially increasing id to create a new column called event_index\n",
        "    return the df\n",
        "    '''\n",
        "    # Hint: pyspark.sql.functions: monotonically_increasing_id\n",
        "    # Output colomns should be (eventid, event_index)\n",
        "    event_map = (agg_events\n",
        "          .select('eventid')\n",
        "          .distinct()\n",
        "          .sort('eventid')\n",
        "          .withColumn('event_index', monotonically_increasing_id())\n",
        "        )\n",
        "\n",
        "    return event_map.select('eventid', 'event_index')\n",
        "\n",
        "def normalization(agg_events):\n",
        "    '''\n",
        "    INPUT:\n",
        "    agg_events\n",
        "    e.g.\n",
        "    +---------+------------+-------------+\n",
        "    |patientid|     eventid|feature_value|\n",
        "    +---------+------------+-------------+\n",
        "    |     5206|DRUG19065818|            1|\n",
        "    |     5206|  LAB3021119|            1|\n",
        "    |    20459|  LAB3013682|           11|\n",
        "\n",
        "\n",
        "    OUTPUT:\n",
        "    normalized\n",
        "    e.g.\n",
        "    +---------+------------+------------------------+\n",
        "    |patientid|     eventid|normalized_feature_value|\n",
        "    +---------+------------+------------------------+\n",
        "    |     5206|DRUG19065818|                   1.000|\n",
        "    |     5206|  LAB3021119|                   1.000|\n",
        "    |    20459|  LAB3013682|                   0.379|\n",
        "    +---------+------------+------------------------+\n",
        "    '''\n",
        "    '''\n",
        "    - Create a df by grouping all events by eventid\n",
        "        and finding the maximum value for each event\n",
        "    - Use this maximum as a normalization map, store the result of the\n",
        "    normalization in a new column in the normalized df\n",
        "    '''\n",
        "    # Output columns should be (patientid, eventid, normalized_feature_value)\n",
        "    # Note: round the normalized_feature_value to 3 places after decimal: use round() in pyspark.sql.functions\n",
        "    event_max = (agg_events\n",
        "                 .groupBy('eventid')\n",
        "                 .agg(\n",
        "                     max_('feature_value')\n",
        "                     .alias('max_value')\n",
        "                     )\n",
        "                 .select('eventid', 'max_value')\n",
        "                 )\n",
        "\n",
        "    normalized = (agg_events\n",
        "          .join(event_max, on='eventid', how='inner')\n",
        "          .withColumn('normalized_feature_value', round(col('feature_value')/col('max_value'), 3))\n",
        "          .select('patientid', 'eventid', 'normalized_feature_value')\n",
        "        ).orderBy('patientid', 'eventid')\n",
        "    return normalized\n",
        "\n",
        "def svmlight_convert(normalized, event_map):\n",
        "    '''\n",
        "    INPUT:\n",
        "    normalized\n",
        "    e.g.\n",
        "    +---------+------------+------------------------+\n",
        "    |patientid|     eventid|normalized_feature_value|\n",
        "    +---------+------------+------------------------+\n",
        "    |    20459|  LAB3023103|                   0.062|\n",
        "    |    20459|  LAB3027114|                   1.000|\n",
        "    |    20459|  LAB3007461|                   0.115|\n",
        "    +---------+------------+------------------------+\n",
        "\n",
        "    event_map\n",
        "    e.g.\n",
        "    +----------+-----------+\n",
        "    |   eventid|event_index|\n",
        "    +----------+-----------+\n",
        "    |DIAG132797|          0|\n",
        "    |DIAG135214|          1|\n",
        "    |DIAG137829|          2|\n",
        "    +----------+-----------+\n",
        "\n",
        "    OUTPUT:\n",
        "    svmlight: patientid, sparse_feature\n",
        "    sparse_feature is a list containing: feature pairs\n",
        "    earch feature pair is a string: \"event_index:normalized_feature_val\"\n",
        "    e.g\n",
        "    +---------+-------------------+\n",
        "    |patientid|   sparse_feature  |\n",
        "    +---------+-------------------+\n",
        "    |    19992|[2:1.000, 9:1.000] |\n",
        "    |    19993|[2:0.667, 12:0.500]|\n",
        "    +---------+-------------------+\n",
        "    '''\n",
        "    # Output columns should be (patientid, sparse_feature)\n",
        "    # Note: for normalized_feature_val, when convert it to string, save 3 digits after decimal including \"0\": use format_number() in pyspark.sql.functions\n",
        "    # Hint:\n",
        "    #         pyspark.sql.functions: concat_with(), collect_list()\n",
        "    #         pyspark.sql.window: Window.partitionBy(), Window.orderBy()\n",
        "    '''\n",
        "    - Join event_map on event_id to get event_index\n",
        "    - Group by patient id\n",
        "        collect all feature numbers and their values as a list\n",
        "        sort the list\n",
        "    - convert the list to a string\n",
        "    - return the patient id and the string\n",
        "    '''\n",
        "    # make a string that has each event_index:feature_value\n",
        "    # create feature strings\n",
        "    svmlight = (normalized\n",
        "        .join(event_map, on='eventid', how='inner') # maps event id to event index\n",
        "        .withColumn(\n",
        "            'index_feature',\n",
        "            # creates a struct to sort index/norm value pairs\n",
        "            struct(\n",
        "                col('event_index'),\n",
        "                concat(\n",
        "                    # make an individual string entry for each base row in\n",
        "                    col('event_index'),\n",
        "                    lit(':'),\n",
        "                    format_number(col('normalized_feature_value'), 3) # truncate\n",
        "                ).alias('feature_pair')\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    '''\n",
        "    select only patient id and pregrouped feature string\n",
        "    groupby patient id\n",
        "    aggregate all feature strings into a list\n",
        "    sorted the list\n",
        "    '''\n",
        "    # aggregate and collect all the feature strings\n",
        "    svmlight = (svmlight\n",
        "        .groupBy('patientid')\n",
        "        .agg(\n",
        "            collect_list(col('index_feature')).alias('index_feature_list')\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # convert to svmlight format\n",
        "    svmlight = (svmlight\n",
        "        .select(\n",
        "            col('patientid'),\n",
        "            sort_array(col('index_feature_list'), asc = True).getField('feature_pair').alias('sparse_feature')\n",
        "        )\n",
        "    ).orderBy('patientid')\n",
        "\n",
        "    return svmlight\n",
        "\n",
        "def svmlight_samples(svmlight, mortality):\n",
        "    '''\n",
        "    INPUT:\n",
        "    svmlight\n",
        "    +---------+--------------------+\n",
        "    |patientid|      sparse_feature|\n",
        "    +---------+--------------------+\n",
        "    |     5206|[4:1.000, 5:1.000...|\n",
        "    |    13905|[1:1.000, 11:1.00...|\n",
        "    |    18676|[0:1.000, 2:1.000...|\n",
        "    |    20301|[10:1.000, 12:1.0...|\n",
        "    |    20459|[136:0.250, 137:1...|\n",
        "    +---------+--------------------+\n",
        "\n",
        "    mortality\n",
        "    +---------+----------+-----+\n",
        "    |patientid|mtimestamp|label|\n",
        "    +---------+----------+-----+\n",
        "    |    13905|2000-01-30|    1|\n",
        "    |    18676|2000-02-03|    1|\n",
        "    |    20301|2002-08-08|    1|\n",
        "    +---------+----------+-----+\n",
        "\n",
        "    OUTPUT\n",
        "    samples\n",
        "    +---------+--------------------+-------------+--------------------+\n",
        "    |patientid|      sparse_feature|other columns|        save_feature|\n",
        "    +---------+--------------------+-------------+--------------------+\n",
        "    |     5206|[4:1.000, 5:1.000...|     ...     |0 4:1.000 5:1.000...|\n",
        "    |    13905|[1:1.000, 11:1.00...|     ...     |1 1:1.000 11:1.00...|\n",
        "    |    18676|[0:1.000, 2:1.000...|     ...     |1 0:1.000 2:1.000...|\n",
        "    |    20301|[10:1.000, 12:1.0...|     ...     |1 10:1.000 12:1.0...|\n",
        "    |    20459|[136:0.250, 137:1...|     ...     |0 136:0.250 137:1...|\n",
        "    +---------+--------------------+-------------+--------------------+\n",
        "    '''\n",
        "    '''\n",
        "    - join the svmlight df to the mortality dataframe with only the\n",
        "        patientid and label selected\n",
        "        fill the rows in the left dataframe that do not have a counterpart\n",
        "        in the mortality dataframe with 0\n",
        "    - concatentate the two rows and save as save_feature\n",
        "    -\n",
        "    '''\n",
        "    # Task: create a new DataFrame by adding a new colum in \"svmlight\".\n",
        "    # New column name is \"save_feature\" which is a String including target\n",
        "    # and sparse feature in SVMLight format;\n",
        "    # New DataFrame name is \"samples\"\n",
        "    # You can have other columns in \"samples\"\n",
        "    # Hint:\n",
        "    #         pyspark.sql.functions: concat_with\n",
        "    samples = (svmlight\n",
        "        .join(\n",
        "            mortality.select('patientid', 'label').distinct(),\n",
        "            on='patientid',\n",
        "            how='left_outer'\n",
        "            )\n",
        "        .fillna(0, subset=['label'])\n",
        "    )\n",
        "\n",
        "    samples = (samples\n",
        "        .withColumn(\n",
        "            'save_feature',\n",
        "            concat(\n",
        "                col('label').cast('string'),\n",
        "                lit(' '),\n",
        "                concat_ws(' ', col('sparse_feature'))\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "    return samples.select('patientid', 'sparse_feature', 'save_feature').orderBy('patientid')\n",
        "\n",
        "\n",
        "def train_test_split(samples, train_path, test_path):\n",
        "\n",
        "    # DO NOT change content below\n",
        "    samples = samples.randomSplit([0.2, 0.8], seed=48)\n",
        "\n",
        "    testing = samples[0].select(samples[0].save_feature)\n",
        "    training = samples[1].select(samples[1].save_feature)\n",
        "\n",
        "    #save training and tesing data\n",
        "    if os.path.exists(train_path):\n",
        "        shutil.rmtree(train_path)\n",
        "\n",
        "    training.write.option(\"escape\",\"\").option(\"quotes\", \"\").option(\"delimiter\",\" \").text(train_path)\n",
        "\n",
        "    if os.path.exists(test_path):\n",
        "        shutil.rmtree(test_path)\n",
        "\n",
        "    testing.write.option(\"escape\",\"\").option(\"quotes\", \"\").option(\"delimiter\",\" \").text(test_path)\n",
        "\n",
        "\n",
        "def main():\n",
        "    '''\n",
        "    CHANGE THE path1 AND path2 WHEN YOU NEEDED, AND SWITCH IT BACK WHEN SUBMITTED\n",
        "    '''\n",
        "\n",
        "    path1 =    '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv'\n",
        "     # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv'   # this path is for test and submission,you need to switch it back when submitted\n",
        "\n",
        "    # path1 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data/events.csv'\n",
        "    # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/data/events.csv'    # this path is used to test your model's performance while you run Part2.3, only use this path in Part2.3\n",
        "\n",
        "    schema1 = StructType([\n",
        "        StructField(\"patientid\", IntegerType(), True),\n",
        "        StructField(\"eventid\", StringType(), True),\n",
        "        StructField(\"eventdesc\", StringType(), True),\n",
        "        StructField(\"timestamp\", StringType(), True),\n",
        "        StructField(\"value\", FloatType(), True)])\n",
        "\n",
        "    path2 =  '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv'\n",
        "     # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv' # this path is for test and submission,you need to switch it back when submitted\n",
        "\n",
        "    # path2 =  '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/data/mortality.csv'\n",
        "      # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/data/mortality.csv'   # this path is used to test your model's performance while you run Part2.3, only use this path in Part2.3\n",
        "\n",
        "    schema2 = StructType([\n",
        "        StructField(\"patientid\", IntegerType(), True),\n",
        "        StructField(\"timestamp\", StringType(), True),\n",
        "        StructField(\"label\", IntegerType(), True)])\n",
        "\n",
        "    events = read_csv(spark, path1, schema1)\n",
        "    events = events.select(events.patientid, events.eventid, to_date(events.timestamp).alias(\"etimestamp\"), events.value)\n",
        "\n",
        "\n",
        "    mortality = read_csv(spark, path2, schema2)\n",
        "    mortality = mortality.select(mortality.patientid, to_date(mortality.timestamp).alias(\"mtimestamp\"), mortality.label)\n",
        "\n",
        "    index_dates = calculate_index_dates(events, mortality)\n",
        "    print('index_dates')\n",
        "    index_dates.show()\n",
        "\n",
        "    filtered = filter_events(events, index_dates)\n",
        "    print('filtered')\n",
        "    filtered.show()\n",
        "\n",
        "    agg_events = aggregate_events(filtered)\n",
        "    print('agg_events')\n",
        "    agg_events.show()\n",
        "\n",
        "    event_map = generate_feature_mapping(agg_events)\n",
        "    print('event_map')\n",
        "    event_map.show()\n",
        "    n_feature = event_map.select('eventid').distinct().count()\n",
        "\n",
        "    normalized = normalization(agg_events)\n",
        "    print('normalized')\n",
        "    normalized.show()\n",
        "\n",
        "    svmlight = svmlight_convert(normalized, event_map)\n",
        "    print('svmlight')\n",
        "    svmlight.show()\n",
        "\n",
        "    samples = svmlight_samples(svmlight, mortality)\n",
        "    print('svmlight samples')\n",
        "    samples.show()\n",
        "\n",
        "    train_test_split(samples,\n",
        "                     '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/deliverables/training',\n",
        "                     '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/deliverables/testing'\n",
        "    )\n",
        "    # '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/deliverables/training', '/content/gdrive/My Drive/bdh-hw2-pyspark-publish_colab/deliverables/testing')\n",
        "\n",
        "    return n_feature\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    n_feature = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGbBQWclj_VA"
      },
      "source": [
        "#### Run this block to get the transform data test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gkawBGNuj_VB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ae25e3-69c2-4775-fc1b-c0d1070c6a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".......\n",
            "----------------------------------------------------------------------\n",
            "Ran 7 tests in 10.889s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed! Total Score: 28/28\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "spark = SparkSession.builder.appName('Read CSV File into DataFrame').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "path1 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv'\n",
        "#'/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/sample_test/sample_events.csv'\n",
        "schema1 = StructType([\n",
        "    StructField(\"patientid\", IntegerType(), True),\n",
        "    StructField(\"eventid\", StringType(), True),\n",
        "    StructField(\"eventdesc\", StringType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "    StructField(\"value\", FloatType(), True)])\n",
        "\n",
        "path2 = '/content/gdrive/MyDrive/GradClasses/BigDataForHealthCare_CS6250/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv'\n",
        "# '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/sample_test/sample_mortality.csv'\n",
        "schema2 = StructType([\n",
        "    StructField(\"patientid\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "    StructField(\"label\", IntegerType(), True)])\n",
        "\n",
        "def read_csv(spark, file, schema):\n",
        "    return spark.read.csv(file, header=False, schema=schema)\n",
        "\n",
        "events = read_csv(spark, path1, schema1)\n",
        "events = events.select(events.patientid, events.eventid, to_date(events.timestamp).alias(\"etimestamp\"), events.value)\n",
        "\n",
        "mortality = read_csv(spark, path2, schema2)\n",
        "mortality = mortality.select(mortality.patientid, to_date(mortality.timestamp).alias(\"mtimestamp\"), mortality.label)\n",
        "\n",
        "class ETLTest(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.events = events\n",
        "        cls.mortality = mortality\n",
        "        cls.score = 0  # Initialize the score\n",
        "\n",
        "    def test_index_dates(self):\n",
        "        # INPUT:\n",
        "        # events_df read from events.csv\n",
        "        # e.g.\n",
        "        # +---------+------------+----------+-----+\n",
        "        # |patientid|     eventid|etimestamp|value|\n",
        "        # +---------+------------+----------+-----+\n",
        "        # |    20459|DIAG42872402|1994-12-04|  1.0|\n",
        "\n",
        "        # mortality_df read from mortality.csv\n",
        "        # +---------+----------+-----+\n",
        "        # |patientid|mtimestamp|label|\n",
        "        # +---------+----------+-----+\n",
        "        # |    13905|2000-01-30|    1|\n",
        "        # |    18676|2000-02-03|    1|\n",
        "        # |    20301|2002-08-08|    1|\n",
        "        # +---------+----------+-----+\n",
        "\n",
        "        # OUTPUT:\n",
        "        # index_df\n",
        "        # index_date is datetime.date format\n",
        "        # e.g.\n",
        "        # +---------+----------+\n",
        "        # |patientid|index_date|\n",
        "        # +---------+----------+\n",
        "        # |    20459|2000-09-19|\n",
        "        # |     5206|2000-08-04|\n",
        "        # |    20301|2002-07-09|\n",
        "        # |    13905|1999-12-31|\n",
        "        # |    18676|2000-01-04|\n",
        "        # +---------+----------+\n",
        "\n",
        "        try:\n",
        "            expected = [\n",
        "                [20459, datetime.date(2000, 9, 19)],\n",
        "                [5206, datetime.date(2000, 8, 4)],\n",
        "                [20301, datetime.date(2002, 7, 9)],\n",
        "                [13905, datetime.date(1999, 12, 31)],\n",
        "                [18676, datetime.date(2000, 1, 4)]\n",
        "            ]\n",
        "\n",
        "            indx_date_df = calculate_index_dates(self.events, self.mortality)\n",
        "            temp = indx_date_df.select([\"patientid\", \"index_date\"]).rdd.map(lambda line: [x for x in line]).collect()\n",
        "            self.assertEqual(len(expected), len(temp), \"Index dates do not match\")\n",
        "            for eve in temp:\n",
        "                self.assertIn(eve, expected, \"Index dates do not match\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_filter_events(self):\n",
        "        # INPUT:\n",
        "        # create events df\n",
        "        # events\n",
        "        # e.g.\n",
        "        # +---------+------------+----------+-----+\n",
        "        # |patientid|     eventid|etimestamp|value|\n",
        "        # +---------+------------+----------+-----+\n",
        "        # |    20459|DIAG42872402|1994-12-04|  1.0|\n",
        "        # create index_date df\n",
        "        # index_dates\n",
        "        # +---------+----------+\n",
        "        # |patientid|index_date|\n",
        "        # +---------+----------+\n",
        "        # |    20459|2000-09-19|\n",
        "\n",
        "        # OUTPUT:\n",
        "        # filtered\n",
        "        # e.g.\n",
        "        # +---------+--------------+----------+-----+----------+---------------+\n",
        "        # |patientid|   eventid    |etimestamp|value|index_date|time_difference|\n",
        "        # +---------+--------------+----------+-----+----------+---------------+\n",
        "        # |    20459|'DIAG42872404'|1999-12-04|  1.0|2000-09-19|              0|\n",
        "        # |    19992|'DIAG42872403'|1995-12-04|  1.0|1996-09-19|              0|\n",
        "        # +---------+--------------+----------+-----+----------+---------------+\n",
        "\n",
        "        try:\n",
        "            events_list = [\n",
        "                (20459, 'DIAG42872402', '1994-12-04', 1.0),\n",
        "                (19992, 'DIAG42872403', '1995-12-04', 1.0),\n",
        "                (20459, 'DIAG42872404', '1999-12-04', 1.0),\n",
        "                (20459, 'DIAG42872405', '2000-12-04', 1.0)\n",
        "            ]\n",
        "            columns_events = [\"patientid\", \"eventid\", \"etimestamp\", \"value\"]\n",
        "            events_df = spark.createDataFrame(data=events_list, schema=columns_events)\n",
        "            index_dates_list = [\n",
        "                (20459, '2000-09-19'),\n",
        "                (19992, '1996-09-19')\n",
        "            ]\n",
        "            columns_index = [\"patientid\", \"index_date\"]\n",
        "            index_df = spark.createDataFrame(data=index_dates_list, schema=columns_index)\n",
        "            events_df = events_df.withColumn(\"etimestamp\", to_date(\"etimestamp\", \"yyyy-MM-dd\"))\n",
        "            index_df = index_df.withColumn(\"index_date\", to_date(\"index_date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "            expected = [[19992, 'DIAG42872403'], [20459, 'DIAG42872404']]\n",
        "            filtered = filter_events(events_df, index_df)\n",
        "            temp = filtered.select([\"patientid\", \"eventid\"]).rdd.map(lambda line: [x for x in line]).collect()\n",
        "\n",
        "            self.assertEqual(len(expected), len(temp), \"Events are not filtered correctly\")\n",
        "            for eve in temp:\n",
        "                self.assertIn(eve, expected, \"Events are not filtered correctly\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_aggregate_events(self):\n",
        "        # INPUT:\n",
        "        # filtered\n",
        "        # e.g.\n",
        "        # +---------+----------+----------+-----+----------+---------------+\n",
        "        # |patientid|   eventid|etimestamp|value|index_date|time_difference|\n",
        "        # +---------+----------+----------+-----+----------+---------------+\n",
        "        # |    20459|LAB3013603|2000-09-19|  0.6|2000-09-19|              0|\n",
        "\n",
        "        # OUTPUT:\n",
        "        # patient_features\n",
        "        # e.g.\n",
        "        # +---------+------------+-------------+\n",
        "        # |patientid|     eventid|feature_value|\n",
        "        # +---------+------------+-------------+\n",
        "        # |     5206|DRUG19065818|            1|\n",
        "        # |     5206|  LAB3021119|            1|\n",
        "        # |    20459|  LAB3013682|           11|\n",
        "        # +---------+------------+-------------+\n",
        "\n",
        "        try:\n",
        "            filtered = [\n",
        "                (19992, 'DIAG42872403', '1995-12-04', 1.0, '2000-09-19', 1751),\n",
        "                (19992, 'DIAG42872403', '1995-12-04', 1.0, '2000-09-19', 1751),\n",
        "                (19992, 'DIAG42872403', '1995-12-04', 1.0, '2000-09-19', 1751),\n",
        "                (19992, 'DIAG42872404', '1995-12-04', 1.0, '2000-09-19', 1751),\n",
        "                (29993, 'DIAG42872403', '1995-12-04', 1.0, '2000-09-19', 1751),\n",
        "                (29993, 'DIAG42872403', '1995-12-04', 1.0, '2000-09-19', 1751)\n",
        "            ]\n",
        "            columns_filtered = [\"patientid\", \"eventid\", \"etimestamp\", \"value\", \"index_date\", \"time_difference\"]\n",
        "            filtered_df = spark.createDataFrame(data=filtered, schema=columns_filtered)\n",
        "\n",
        "            expected = [\n",
        "                [19992, 'DIAG42872403', 3],\n",
        "                [29993, 'DIAG42872403', 2],\n",
        "                [19992, 'DIAG42872404', 1]\n",
        "            ]\n",
        "            patient_features = aggregate_events(filtered_df)\n",
        "            temp = patient_features.select([\"patientid\", \"eventid\", \"feature_value\"]).rdd.map(lambda line: [x for x in line]).collect()\n",
        "\n",
        "            self.assertEqual(len(expected), len(temp), \"Features are not created correctly\")\n",
        "            for feat in temp:\n",
        "                self.assertIn(feat, expected, \"Features are not created correctly\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_feature_mapping(self):\n",
        "        # INPUT:\n",
        "        # normalized\n",
        "        # e.g.\n",
        "        # +---------+------------+------------------------+\n",
        "        # |patientid|     eventid|normalized_feature_value|\n",
        "        # +---------+------------+------------------------+\n",
        "        # |     5206|DRUG19065818|                   1.000|\n",
        "        # |     5206|  LAB3021119|                   1.000|\n",
        "        # |    20459|  LAB3013682|                   0.379|\n",
        "        # +---------+------------+------------------------+\n",
        "\n",
        "        # OUTPUT:\n",
        "        # event_map\n",
        "        # e.g.\n",
        "        # +------------+---------+------------------------+-----------+\n",
        "        # |     eventid|patientid|normalized_feature_value|event_index|\n",
        "        # +------------+---------+------------------------+-----------+\n",
        "        # |DRUG19065818|     5206|                   1.000|         81|\n",
        "        # |  LAB3021119|     5206|                   1.000|        174|\n",
        "        # |  LAB3013682|    20459|                   0.379|        157|\n",
        "        # +------------+---------+------------------------+-----------+\n",
        "\n",
        "        try:\n",
        "            normalized = [\n",
        "                (19992, 'DIAG42872403', 1.000),\n",
        "                (19992, 'DIAG42872404', 1.000),\n",
        "                (19993, 'DIAG42872403', 0.667),\n",
        "                (19993, 'LAB1234', 0.667)\n",
        "            ]\n",
        "            columns_normalized = [\"patientid\", \"eventid\", \"normalized_feature_value\"]\n",
        "            normalized_df = spark.createDataFrame(data=normalized, schema=columns_normalized)\n",
        "\n",
        "            expected = [('DIAG42872403', 0), ('DIAG42872404', 1), ('LAB1234', 2)]\n",
        "            mapping = generate_feature_mapping(normalized_df)\n",
        "            temp = mapping.rdd.map(lambda x: (x['eventid'], int(x['event_index']))).collect()\n",
        "\n",
        "            self.assertEqual(len(expected), len(temp), \"Feature mapping is not correct!\")\n",
        "            for feat in temp:\n",
        "                self.assertIn(feat, expected, \"Feature mapping is not correct!\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_normalization(self):\n",
        "        # INPUT:\n",
        "        # patient_features\n",
        "        # e.g.\n",
        "        # +---------+------------+-------------+\n",
        "        # |patientid|     eventid|feature_value|\n",
        "        # +---------+------------+-------------+\n",
        "        # |     5206|DRUG19065818|            1|\n",
        "        # |     5206|  LAB3021119|            1|\n",
        "        # |    20459|  LAB3013682|           11|\n",
        "\n",
        "\n",
        "        # OUTPUT:\n",
        "        # normalized\n",
        "        # e.g.\n",
        "        # +---------+------------+------------------------+\n",
        "        # |patientid|     eventid|normalized_feature_value|\n",
        "        # +---------+------------+------------------------+\n",
        "        # |     5206|DRUG19065818|                   1.000|\n",
        "        # |     5206|  LAB3021119|                   1.000|\n",
        "        # |    20459|  LAB3013682|                   0.379|\n",
        "        # +---------+------------+------------------------+\n",
        "\n",
        "        try:\n",
        "            features = [\n",
        "                (19992, 'DIAG42872403', 1),\n",
        "                (19992, 'DIAG42872404', 999),\n",
        "                (19993, 'DIAG42872403', 4)\n",
        "            ]\n",
        "            columns_features = [\"patientid\", \"eventid\", \"feature_value\"]\n",
        "            features_df = spark.createDataFrame(data=features, schema=columns_features)\n",
        "\n",
        "            expected = [\n",
        "                [19992, 'DIAG42872403', 0.250],\n",
        "                [19993, 'DIAG42872403', 1.000],\n",
        "                [19992, 'DIAG42872404', 1.000]\n",
        "            ]\n",
        "            normalized = normalization(features_df)\n",
        "            temp = normalized.select([\"patientid\", \"eventid\", \"normalized_feature_value\"]).rdd.map(lambda x: [int(x[0]), x[1], x[2]]).collect()\n",
        "            self.assertEqual(len(expected), len(temp), \"Normalization is not correct!\")\n",
        "            expected_dict = {tuple(e[:2]): e[2] for e in expected}\n",
        "            for feat in temp:\n",
        "                k = tuple(feat[:2])\n",
        "                self.assertIn(k, expected_dict, f\"No such patient and event: {feat[0]}, {feat[1]}\")\n",
        "                self.assertAlmostEqual(expected_dict[k], feat[2], places=2, msg=f\"UNEQUAL in normalized val, Expected:{expected_dict[k]}, Actual:{feat[2]}\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_svmlight_convert(self):\n",
        "        # INPUT:\n",
        "        # normalized\n",
        "        # e.g.\n",
        "        # +---------+------------+------------------------+\n",
        "        # |patientid|     eventid|normalized_feature_value|\n",
        "        # +---------+------------+------------------------+\n",
        "        # |    20459|  LAB3023103|                   0.062|\n",
        "        # |    20459|  LAB3027114|                   1.000|\n",
        "        # |    20459|  LAB3007461|                   0.115|\n",
        "        # +---------+------------+------------------------+\n",
        "\n",
        "        # event_map\n",
        "        # e.g.\n",
        "        # +----------+-----------+\n",
        "        # |   eventid|event_index|\n",
        "        # +----------+-----------+\n",
        "        # |DIAG132797|          0|\n",
        "        # |DIAG135214|          1|\n",
        "        # |DIAG137829|          2|\n",
        "        # +----------+-----------+\n",
        "\n",
        "        # OUTPUT:\n",
        "        # svmlight: patientid, sparse_feature\n",
        "        # sparse_feature is a list containing: features\n",
        "        # earch feature is a string: \"event_index:normalized_feature_val\"\n",
        "        # e.g\n",
        "        # +---------+-------------------+\n",
        "        # |patientid|   sparse_feature  |\n",
        "        # +---------+-------------------+\n",
        "        # |    19992|[2:1.000, 9:1.000] |\n",
        "        # |    19993|[2:0.667, 12:0.500]|\n",
        "        # +---------+-------------------+\n",
        "\n",
        "        try:\n",
        "            normalized = [\n",
        "                (19992, 'DIAG4', 1.000),\n",
        "                (19992, 'DIAG8', 1.000),\n",
        "                (19993, 'LAB3', 0.500),\n",
        "                (19993, 'DIAG4', 0.667)\n",
        "            ]\n",
        "            columns_normalized = [\"patientid\", \"eventid\", \"normalized_feature_value\"]\n",
        "            normalized_df = spark.createDataFrame(data=normalized, schema=columns_normalized)\n",
        "            mapping = [('DIAG4', 2), ('DIAG8', 9), ('LAB3', 12)]\n",
        "            columns_mapping = [\"eventid\", \"event_index\"]\n",
        "            mapping_df = spark.createDataFrame(data=mapping, schema=columns_mapping)\n",
        "\n",
        "            expected = {19992: [\"2:1.000\", \"9:1.000\"], 19993: [\"2:0.667\", \"12:0.500\"]}\n",
        "            svmlight = svmlight_convert(normalized_df, mapping_df)\n",
        "            temp = svmlight.rdd.map(lambda x: (x['patientid'], x['sparse_feature'])).collect()\n",
        "\n",
        "            self.assertEqual(len(expected), len(temp), \"Svmlight conversion is not correct!\")\n",
        "            for pid, feat in temp:\n",
        "                self.assertIn(pid, expected, \"Svmlight conversion is not correct!\")\n",
        "                self.assertEqual(feat, expected[pid], \"Svmlight conversion is not correct!\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    def test_svmlight_samples(self):\n",
        "        # INPUT:\n",
        "        # svmlight\n",
        "        # +---------+--------------------+\n",
        "        # |patientid|      sparse_feature|\n",
        "        # +---------+--------------------+\n",
        "        # |     5206|[4:1.000, 5:1.000...|\n",
        "        # |    13905|[1:1.000, 11:1.00...|\n",
        "        # |    18676|[0:1.000, 2:1.000...|\n",
        "        # |    20301|[10:1.000, 12:1.0...|\n",
        "        # |    20459|[136:0.250, 137:1...|\n",
        "        # +---------+--------------------+\n",
        "\n",
        "        # mortality\n",
        "        # +---------+----------+-----+\n",
        "        # |patientid|mtimestamp|label|\n",
        "        # +---------+----------+-----+\n",
        "        # |    13905|2000-01-30|    1|\n",
        "        # |    18676|2000-02-03|    1|\n",
        "        # |    20301|2002-08-08|    1|\n",
        "        # +---------+----------+-----+\n",
        "\n",
        "        # Task: create a new DataFrame by adding a new colum in \"svmlight\".\n",
        "        # New column name is \"save_feature\" which is a String including target\n",
        "        # and sparse feature in SVMLight format;\n",
        "        # New DataFrame name is \"samples\"\n",
        "        # You can have other columns in \"samples\"\n",
        "\n",
        "        # OUTPUT\n",
        "        # samples\n",
        "        # +---------+--------------------+-------------+--------------------+\n",
        "        # |patientid|      sparse_feature|other columns|        save_feature|\n",
        "        # +---------+--------------------+-------------+--------------------+\n",
        "        # |     5206|[4:1.000, 5:1.000...|     ...     |0 4:1.000 5:1.000...|\n",
        "        # |    13905|[1:1.000, 11:1.00...|     ...     |1 1:1.000 11:1.00...|\n",
        "        # |    18676|[0:1.000, 2:1.000...|     ...     |1 0:1.000 2:1.000...|\n",
        "        # |    20301|[10:1.000, 12:1.0...|     ...     |1 10:1.000 12:1.0...|\n",
        "        # |    20459|[136:0.250, 137:1...|     ...     |0 136:0.250 137:1...|\n",
        "        # +---------+--------------------+-------------+--------------------+\n",
        "        # Hint:\n",
        "        #         pyspark.sql.functions: concat_with\n",
        "\n",
        "        try:\n",
        "            svmlight_data = [\n",
        "                (19993, [\"1:0.500\", \"97:0.667\"]),\n",
        "                (19992, [\"2:1.000\", \"2001:1.000\"])\n",
        "            ]\n",
        "            columns_svmlight = [\"patientid\", \"sparse_feature\"]\n",
        "            svmlight_df = spark.createDataFrame(data=svmlight_data, schema=columns_svmlight)\n",
        "            mortality_data = [(19993, datetime.date(2000, 9, 19), 1)]\n",
        "            columns_mortality = [\"patientid\", \"mtimestamp\", \"label\"]\n",
        "            mortality_df = spark.createDataFrame(data=mortality_data, schema=columns_mortality)\n",
        "\n",
        "            expected = [\n",
        "                [19992, \"0 2:1.000 2001:1.000\"],\n",
        "                [19993, \"1 1:0.500 97:0.667\"]\n",
        "            ]\n",
        "            samples = svmlight_samples(svmlight_df, mortality_df)\n",
        "            temp = samples.rdd.map(lambda x: [x['patientid'], x['save_feature']]).collect()\n",
        "\n",
        "            self.assertEqual(len(expected), len(temp), \"Svmlight feature string is not correct!\")\n",
        "            for feat in temp:\n",
        "                self.assertIn(feat, expected, \"Svmlight feature string is not correct!\")\n",
        "            ETLTest.score += 4  # Increment score\n",
        "        except AssertionError:\n",
        "            pass\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        if cls.score == 28:  # All tests passed\n",
        "            print(f\"All tests passed! Total Score: {cls.score}/28\")\n",
        "        else:\n",
        "            print(f\"Total Score: {cls.score}/28\")\n",
        "        df_score.loc[df_score['Test'] == 'ETLTest', 'Score'] = ETLTest.score\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create a test suite\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(ETLTest)\n",
        "\n",
        "    # Run the test suite\n",
        "    unittest.TextTestRunner().run(suite)\n",
        "    df_score.loc[df_score['Test'] == 'ETLTest', 'Score'] = ETLTest.score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvHgwmVij_VB"
      },
      "source": [
        "# 2.3 SGD Logistic Regression [25 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBtMRWIpj_VB"
      },
      "outputs": [],
      "source": [
        "# Do not use anything outside of the standard distribution of python\n",
        "# when implementing this class\n",
        "import math\n",
        "import pickle\n",
        "import sys\n",
        "from optparse import OptionParser\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Logistic Regression SGD Implementation\n",
        "class LogisticRegressionSGD:\n",
        "    \"\"\"\n",
        "    Logistic regression with stochastic gradient descent\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eta, mu, n_feature):\n",
        "        \"\"\"\n",
        "        Initialization of model parameters\n",
        "        \"\"\"\n",
        "        self.eta = eta\n",
        "        self.weight = [0.0] * n_feature\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Update model using a pair of training sample\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict 0 or 1 given X and the current weights in the model\n",
        "        \"\"\"\n",
        "        return 1 if self.predict_prob(X) > 0.5 else 0\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        \"\"\"\n",
        "        Sigmoid function\n",
        "        \"\"\"\n",
        "        return 1.0 / (1.0 + math.exp(-math.fsum((self.weight[f]*v for f, v in X))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GP6XM6Hj_VC"
      },
      "source": [
        "#### The following code trains the model you develop above, it should not be changed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieeajo0wj_VC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Function to merge multiple text files into a single file\n",
        "def merge_files(input_folder, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for fname in os.listdir(input_folder):\n",
        "            if fname.startswith('part-') and fname.endswith('.txt'):\n",
        "                with open(os.path.join(input_folder, fname)) as infile:\n",
        "                    for line in infile:\n",
        "                        outfile.write(line)\n",
        "\n",
        "# Merge training data files into train.svmlight\n",
        "merge_files('/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/deliverables/training', '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/data/train.svmlight')\n",
        "# Merge testing data files into test.svmlight\n",
        "merge_files('/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/deliverables/testing', '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/data/test.svmlight')\n",
        "\n",
        "# Utility functions\n",
        "to_float_tuple = lambda x: (int(x[0]), float(x[1]))\n",
        "\n",
        "def parse_svm_light_data(input_file):\n",
        "    with open(input_file, 'r') as f:\n",
        "        for line in f:\n",
        "            yield parse_svm_light_line(line)\n",
        "\n",
        "def parse_svm_light_line(line):\n",
        "    splits = line.split()\n",
        "\n",
        "    y = float(splits[0])\n",
        "    X = []\n",
        "    if len(splits) > 1:\n",
        "        X = [to_float_tuple(kv.split(':')) for kv in splits[1:]]\n",
        "    else:\n",
        "        X = []\n",
        "    return (X, y)\n",
        "\n",
        "# Training script\n",
        "def train_script(train_data_path, eta, mu, n_feature, model_path):\n",
        "    classifier = LogisticRegressionSGD(eta, mu, n_feature)\n",
        "    for X, y in parse_svm_light_data(train_data_path):\n",
        "        classifier.fit(X, y)\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(classifier, f, protocol=0)\n",
        "    return classifier\n",
        "\n",
        "# Testing script\n",
        "def test_script(test_data_path, model_path, result_path):\n",
        "    with open(model_path, 'rb') as f:\n",
        "        classifier = pickle.load(f)\n",
        "        y_test_prob = []\n",
        "        y_test = []\n",
        "        for X, y in parse_svm_light_data(test_data_path):\n",
        "            y_prob = classifier.predict_prob(X)\n",
        "            y_test.append(y)\n",
        "            y_test_prob.append(y_prob)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver operating characteristic')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(result_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ-Hi_ADj_VC"
      },
      "source": [
        "#### Train and evaluate the model and draw the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YthGP98yj_VD"
      },
      "outputs": [],
      "source": [
        "# You may run this multiple times\n",
        "# to try different learning rates eta and regularization parameters mu\n",
        "\n",
        "train_data_path = '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/data/train.svmlight'\n",
        "test_data_path = '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/data/test.svmlight'\n",
        "\n",
        "model_folder = '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/models'\n",
        "result_folder = '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/results'\n",
        "\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "os.makedirs(result_folder, exist_ok=True)\n",
        "\n",
        "eta = 0.01 # Learning rate\n",
        "mu = 0.0 # Regularization parameter\n",
        "\n",
        "model_path = os.path.join(model_folder, f'model_eta{eta}_mu{mu}.pkl')\n",
        "result_path = os.path.join(result_folder, f'roc_eta{eta}_mu{mu}.png')\n",
        "\n",
        "# Train the model\n",
        "classifier = train_script(train_data_path, eta, mu, n_feature, model_path)\n",
        "\n",
        "# Evaluate the model\n",
        "test_script(test_data_path, model_path, result_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Coi6cb-sj_VD"
      },
      "source": [
        "#### After finish drawing, run here to download all the ROC curve images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5FdNlrij_VD"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/results', 'zip', '/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/results')\n",
        "files.download('/content/gdrive/MyDrive/bdh-hw2-pyspark-publish_colab/results.zip')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}